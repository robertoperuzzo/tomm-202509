# ADR-015: Indexer Chunking Format Incompatibility

## Status

Accepted - Phase 1 Completed

## Context

Following the implementation of ADR-014 (Chunking Demo Separate Output Files), the chunking process has been refactored to generate separate output files for each strategy-parser-document combination instead of consolidated files. This refactoring has created an incompatibility with the existing indexer system in `src/indexer`.

### Previous Chunking Format (Before ADR-014)

The old chunking system generated consolidated files with the following structure:

```json
{
  "document_info": {
    "document_id": "9308101_DynamicBacktracking",
    "title": "Document Title",
    "source_file": "path/to/source"
  },
  "results": {
    "fixed_size": {
      "chunks": [
        {
          "content": "chunk content",
          "metadata": {...}
        }
      ],
      "statistics": {...}
    },
    "semantic": {
      "chunks": [...],
      "statistics": {...}
    },
    "sliding_langchain": {
      "chunks": [...],
      "statistics": {...}
    }
  }
}
```

**File naming pattern:** `chunking_demo_results_{document_id}.json`

### Current Chunking Format (After ADR-014)

The new chunking system generates separate files per strategy-parser combination:

```json
{
  "document_info": {
    "document_id": "9308101_DynamicBacktracking",
    "title": "9308101 Dynamic Backtracking",
    "content_length": 63977,
    "source_file": "/workspace/data/processed/pypdf/9308101_DynamicBacktracking_20250905_103823.json",
    "preprocessing_method": "pypdf"
  },
  "strategy_config": {
    "strategy_name": "fixed_size",
    "parameters": {
      "chunk_size": 512,
      "overlap": 50
    }
  },
  "results": {
    "chunks": [
      {
        "content": "chunk content",
        "metadata": {...}
      }
    ]
  },
  "processing_metadata": {
    "timestamp": "ISO-8601",
    "processing_time": 2.14,
    "memory_usage": 45.2,
    "cpu_usage_percent": 12.3,
    "gpu_usage_percent": 0.0,
    "gpu_memory_usage": 0.0
  }
}
```

**File naming pattern:** `{document_id}_{preprocessing_method}_{chunking_strategy}.json`

### Indexer Incompatibilities

The indexer system (`src/indexer/data_processor.py`) has multiple incompatibilities with the new format:

#### 1. File Discovery Issue

**Current implementation:**

```python
def get_chunks_file(self, document_id: str) -> Optional[Path]:
    pattern = f"*{document_id}*.json"
    chunks_files = list(self.chunks_data_path.glob(pattern))

    if len(chunks_files) > 1:
        logger.warning(f"Multiple chunks files found for {document_id}, using first: {chunks_files[0]}")

    return chunks_files[0]
```

**Problem:** With the new format, there are multiple files per document (one per strategy-preprocessing combination). The indexer arbitrarily picks the first one and logs a warning, but this doesn't align with the strategy-specific processing model.

#### 2. Strategy Discovery Issue

**Current implementation:**

```python
def get_available_strategies(self, chunks_data: Dict[str, Any]) -> List[str]:
    if 'results' not in chunks_data:
        return []
    return list(chunks_data['results'].keys())
```

**Problem:** The indexer expects `chunks_data['results']` to contain strategy names as keys. In the new format, `chunks_data['results']` directly contains the `chunks` array.

#### 3. Strategy Data Access Issue

**Current implementation:**

```python
if chunking_strategy not in chunks_data['results']:
    logger.warning(f"Strategy {chunking_strategy} not found in chunks data for {document_id}")
    continue

strategy_data = chunks_data['results'][chunking_strategy]
```

**Problem:** The indexer expects `chunks_data['results'][strategy_name]` to contain the strategy data. In the new format, the strategy data is directly in `chunks_data['results']`.

#### 4. Data Processing Flow Issue

**Current flow:**

1. Get processed files for extraction method
2. For each processed file, extract document_id
3. Find a single chunks file for that document_id
4. Extract strategy data from `chunks_data['results'][strategy_name]`

**New format requires:**

1. Get processed files for extraction method
2. For each processed file, extract document_id
3. Find the specific chunks file for that document_id + extraction_method + chunking_strategy combination
4. Extract strategy data from `chunks_data['results']` directly

### Impact Analysis

The indexer system currently cannot:

1. **Index new chunk files**: The format mismatch prevents successful indexing
2. **Support multi-strategy processing**: Cannot process multiple strategies for the same document
3. **Handle preprocessing method awareness**: No mechanism to select chunks based on preprocessing method
4. **Process performance metadata**: New processing_metadata is not utilized

### Affected Components

- `src/indexer/data_processor.py` - Core data processing logic
- `src/indexer/cli.py` - CLI interface for indexing operations
- `src/indexer/typesense_indexer.py` - Actual indexing implementation
- `scripts/reindex.sh` - Reindexing automation script

## Decision

We will refactor the indexer system to support the new chunking format while maintaining backward compatibility where possible. The refactoring will address the file discovery, strategy processing, and data access patterns to align with the new separate-files approach.

### Proposed Changes

#### 1. Enhanced File Discovery

Replace the current `get_chunks_file` method with a strategy-aware version:

```python
def get_chunks_file(self, document_id: str, extraction_method: str, chunking_strategy: str) -> Optional[Path]:
    """Get chunks file for a specific document-extraction-strategy combination."""
    pattern = f"{document_id}_{extraction_method}_{chunking_strategy}.json"
    chunks_file = self.chunks_data_path / pattern

    if chunks_file.exists():
        return chunks_file

    # Fallback to old pattern for backward compatibility
    old_pattern = f"*{document_id}*.json"
    old_files = list(self.chunks_data_path.glob(old_pattern))
    if old_files:
        logger.info(f"Using legacy chunks file format for {document_id}")
        return old_files[0]

    logger.warning(f"No chunks file found for {document_id}_{extraction_method}_{chunking_strategy}")
    return None
```

#### 2. Format-Aware Data Processing

Implement format detection and appropriate processing:

```python
def load_chunks_data(self, file_path: Path, chunking_strategy: str = None) -> Optional[Dict[str, Any]]:
    """Load chunks data with format auto-detection."""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Detect format based on structure
        if 'results' in data and 'chunks' in data['results']:
            # New format: direct chunks access
            return data
        elif 'results' in data and chunking_strategy in data['results']:
            # Old format: strategy-keyed access
            # Transform to new format for consistent processing
            strategy_data = data['results'][chunking_strategy]
            return {
                'document_info': data.get('document_info', {}),
                'strategy_config': strategy_data.get('config', {}),
                'results': {
                    'chunks': strategy_data.get('chunks', [])
                }
            }
        else:
            logger.error(f"Unrecognized chunks data format in {file_path}")
            return None

    except Exception as e:
        logger.error(f"Error loading chunks data {file_path}: {e}")
        return None
```

#### 3. Strategy Discovery Refactoring

Update strategy discovery to work with both formats:

```python
def get_available_strategies_for_document(self, document_id: str, extraction_method: str) -> List[str]:
    """Get available strategies for a specific document-extraction combination."""
    strategies = []

    # Check for new format files
    for strategy in ['fixed_size', 'semantic', 'sliding_langchain', 'sliding_unstructured']:
        pattern = f"{document_id}_{extraction_method}_{strategy}.json"
        if (self.chunks_data_path / pattern).exists():
            strategies.append(strategy)

    # If no new format files, check for old format
    if not strategies:
        old_pattern = f"*{document_id}*.json"
        old_files = list(self.chunks_data_path.glob(old_pattern))
        if old_files:
            # Load old format and extract strategies
            chunks_data = self.load_chunks_data(old_files[0])
            if chunks_data and 'results' in chunks_data:
                strategies = list(chunks_data['results'].keys())

    return strategies
```

#### 4. Processing Pipeline Updates

Modify the main processing logic:

```python
def prepare_documents_for_indexing(self, extraction_method: str, chunking_strategy: str, max_documents: int = -1) -> List[Dict[str, Any]]:
    """Prepare documents for indexing with new format support."""
    documents = []
    processed_files = self.get_processed_files(extraction_method, max_documents)

    for file_path in processed_files:
        processed_doc = self.load_processed_document(file_path)
        if not processed_doc:
            continue

        document_id = processed_doc.get('document_id')
        if not document_id:
            continue

        # Use strategy-aware file discovery
        chunks_file = self.get_chunks_file(document_id, extraction_method, chunking_strategy)
        if not chunks_file:
            continue

        chunks_data = self.load_chunks_data(chunks_file, chunking_strategy)
        if not chunks_data:
            continue

        # Process chunks (same logic as before, but simpler access)
        if 'results' not in chunks_data or 'chunks' not in chunks_data['results']:
            logger.warning(f"No chunks in data for {document_id}")
            continue

        for chunk in chunks_data['results']['chunks']:
            doc = self._create_index_document(
                processed_doc, chunk, extraction_method, chunking_strategy, chunks_data
            )
            if doc:
                documents.append(doc)

    return documents
```

#### 5. Enhanced Document Creation

Update document creation to utilize new metadata:

```python
def _create_index_document(self, processed_doc: Dict[str, Any], chunk: Dict[str, Any],
                          extraction_method: str, chunking_strategy: str,
                          chunks_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Create a document for indexing with enhanced metadata."""
    try:
        # Get strategy config from new format
        strategy_config = chunks_data.get('strategy_config', {})
        processing_metadata = chunks_data.get('processing_metadata', {})

        doc = {
            # ... existing fields ...

            # New fields from enhanced format
            "preprocessing_method": chunks_data.get('document_info', {}).get('preprocessing_method', extraction_method),
            "content_length": chunks_data.get('document_info', {}).get('content_length', 0),
            "processing_time": processing_metadata.get('processing_time', 0),
            "memory_usage": processing_metadata.get('memory_usage', 0),
            "cpu_usage_percent": processing_metadata.get('cpu_usage_percent', 0),
            "gpu_usage_percent": processing_metadata.get('gpu_usage_percent', 0),
        }

        # Add strategy parameters
        if 'parameters' in strategy_config:
            params = strategy_config['parameters']
            doc.update({
                "chunk_size": params.get('chunk_size', 0),
                "chunk_overlap": params.get('overlap', 0),
                "encoding_name": params.get('encoding_name', ''),
            })

        return doc

    except Exception as e:
        logger.error(f"Error creating index document: {e}")
        return None
```

### Migration Strategy

1. **Backward Compatibility**: Support both old and new formats during transition period
2. **Gradual Migration**: Update indexer first, then migrate chunk files
3. **Validation**: Add validation to ensure both formats work correctly
4. **Documentation**: Update indexer documentation and usage guides

## Consequences

### Positive

- **Format Alignment**: Indexer will work with the new chunking format
- **Enhanced Metadata**: Can utilize performance and processing metadata for analysis
- **Strategy Granularity**: Better support for strategy-specific processing
- **Preprocessing Awareness**: Can differentiate between different preprocessing methods
- **Backward Compatibility**: Maintains support for existing chunk files during migration
- **Performance Insights**: Can index and query performance metrics

### Negative

- **Implementation Complexity**: More complex file discovery and format handling logic
- **Migration Effort**: Requires careful migration of existing indexed data
- **Testing Overhead**: Need to test both old and new format support
- **Temporary Dual Support**: Maintaining backward compatibility adds code complexity

### Neutral

- **File Structure**: New file organization requires updates but provides better granularity
- **Processing Logic**: More explicit strategy-file mapping but clearer relationships

## Implementation Plan

### Phase 1: Core Refactoring ✅ COMPLETED

**Implementation Date:** September 10, 2025

**Changes Made:**

1. ✅ **Updated `DataProcessor.get_chunks_file()`** for strategy-aware discovery

   - Added optional `extraction_method` and `chunking_strategy` parameters
   - Implements new format file discovery: `{document_id}_{extraction_method}_{chunking_strategy}.json`
   - Maintains backward compatibility with old format fallback
   - Enhanced logging for format detection

2. ✅ **Implemented format detection in `load_chunks_data()`**

   - Auto-detects new vs old chunk file format
   - Transforms old format to new format structure for consistent processing
   - Handles both `chunks_data['results']['chunks']` (new) and `chunks_data['results'][strategy_name]['chunks']` (old)
   - Improved error handling with specific exception types

3. ✅ **Updated `prepare_documents_for_indexing()`** processing logic

   - Uses strategy-aware file discovery
   - Simplified chunk access path (always `chunks_data['results']['chunks']`)
   - Passes chunks_data to document creation for enhanced metadata
   - Improved logging with lazy formatting

4. ✅ **Enhanced `_create_index_document()`** with new metadata
   - Added optional `chunks_data` parameter for new format metadata
   - Extracts and indexes performance metrics (processing_time, memory_usage, cpu_usage_percent, gpu_usage_percent)
   - Includes preprocessing method and content length from document_info
   - Maintains backward compatibility for old format strategy configs
   - Prioritizes new format strategy parameters over old format

**Testing Results:**

- ✅ **New Format Support**: Successfully indexes new separate chunk files
- ✅ **Backward Compatibility**: Falls back to old format when new format not available
- ✅ **Multi-Strategy Support**: Correctly handles different chunking strategies (fixed_size, semantic, sliding_langchain)
- ✅ **Multi-Method Support**: Works with different preprocessing methods (pypdf, unstructured)
- ✅ **Enhanced Metadata**: Successfully extracts and indexes performance metrics
- ✅ **CLI Integration**: All existing CLI commands work with refactored code
- ✅ **Error Handling**: Graceful handling of missing files and format mismatches

**Performance:**

- Indexed 27 documents (pypdf/fixed_size) in ~1 second
- Indexed 342 documents (pypdf/semantic) in ~7 seconds
- Indexed 18 documents (unstructured/fixed_size) in ~1 second
- No performance degradation observed

**Files Modified:**

- `src/indexer/data_processor.py` - Core refactoring implemented

### Phase 2: Performance Metadata Enhancement

**Scope:** Implement enhanced performance metadata indexing and querying capabilities

**Changes:**

1. **Performance Metadata Indexing Enhancement**
   - Create specialized indexing capabilities for performance analytics
   - Enable aggregation queries (e.g., "find fastest chunking strategy for documents over 50KB")
   - Support for comparative analytics across different document types and strategies
   - Add performance-focused search and filtering capabilities

**Goals:**

- Enable performance-based decision making for chunking strategies
- Support research and optimization of chunking approaches
- Provide insights into resource usage patterns across different strategies

### Phase 3: Future Considerations (Optional)

**Note:** Migration support and advanced CLI features have been deemed unnecessary for this project scope.

**Potential future enhancements:**

- Advanced CLI filtering options
- Automated strategy discovery
- Format validation utilities

### Phase 4: Cleanup (Future)

**Scope:** Remove backward compatibility code after new format is fully established

**Changes:**

1. **Remove Old Format Support**

   - Delete backward compatibility code for old format detection and processing
   - Remove format transformation logic in `load_chunks_data()`
   - Eliminate fallback patterns in `get_chunks_file()`

2. **Simplify Code Structure**

   - Make `extraction_method` and `chunking_strategy` parameters required (no longer optional)
   - Remove conditional branches for format differences
   - Streamline error handling for single format

3. **Update Tests and Documentation**

   - Remove tests that verify old format compatibility
   - Update API documentation to reflect simplified method signatures
   - Remove references to legacy format support

4. **Performance Optimization**
   - Eliminate format detection overhead
   - Optimize file discovery (no glob fallback patterns)
   - Reduce memory allocation from format transformation
   - Improve CPU cache utilization with fewer conditional branches

**Prerequisites:**

- Confirmation that all old format files have been removed or migrated
- Validation that no systems depend on old format support
- Performance analysis showing measurable benefits from cleanup

**Benefits:**

- Cleaner, more maintainable codebase
- Better performance (no format detection overhead)
- Reduced testing complexity
- Simplified debugging and profiling

## References

- **Related ADR**: ADR-014 (Chunking Demo Separate Output Files)
- **Affected Files**:
  - `src/indexer/data_processor.py`
  - `src/indexer/cli.py`
  - `src/indexer/typesense_indexer.py`
  - `scripts/reindex.sh`
- **Data Locations**:
  - `data/chunks/` - New format chunk files
  - `data/processed/` - Preprocessed documents
- **Configuration**: `src/indexer/config.py`
- **Documentation**: `src/indexer/README_REINDEX.md`, `src/indexer/REINDEX_GUIDE.md`
